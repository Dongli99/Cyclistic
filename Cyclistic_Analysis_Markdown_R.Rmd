---
title: "Cyclistic Analysis Workflow Using R"
author: "Dongli Liu"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(repos = "https://mirror.rcg.sfu.ca/mirror/CRAN/")
```

## INTRODUCTION {.tabset}

### Usage
> To recurrent this case studay:
*Considering the size of the dataset, this version may be better to read only.*
*A [Kaggle version](https://www.kaggle.com/dongliai/cyclistic-analysis-workflow-using-r) is much easier to recurrent.*

  - to run this version locally, pull [repository](https://github.com/Dongli99/Cyclistic.git)
  - download [dataset](https://centennialcollegeedu-my.sharepoint.com/:u:/g/personal/dliu99_my_centennialcollege_ca/EebPZIYzXXZAr0xgrzCrjkMBBJ1KGVEqkI1mHfyY7_SEqA?e=YmxcDV)
  - extract tripdata/ to Cyclistic/
  - run Cyclistic_Analysis_Markdown_R.Rmd in RStudio
  - expect long-time running due to large dataset.
  
> How to use 

```{r normal_modules, eval=FALSE}
print("This is a normal code chunk.")
```

```{r reusable_module, class.source="reusable_module", eval=FALSE}
print("This is a reusable code chunk.")
```

---

### Overview
This project focuses on the completion of the [Google Data Analytics Professional Certificate](https://www.coursera.org/professional-certificates/google-data-analytics?) case study. The case study involved real-world data analysis tasks within a fictional company, Cyclistic, a bike-share program operating in Chicago. The goal was to analyze data, answer critical business questions, and make data-driven recommendations to maximize annual memberships.

---

### Process
The project followed a structured data analysis process:

- ***Ask***: Defined key business questions and objectives.
- ***Prepare***: Gathered and cleaned the data from various sources.
- ***Process***: Performed data manipulation and transformation for analysis.
- ***Analyze***: Conducted in-depth analysis to uncover insights.
- ***Share***: Communicated findings through visualizations and reports.
- ***Act***: Presented recommendations based on data insights.

---

### Principles
- ***Reusability***: Creating and collecting reusable modules for future applications.
- ***Optimization***: Continuously enhancing modules through study and work.
- ***Structured Organization***: Implementing an organized framework for knowledge and tools.
- ***Thoroughness***: Trying to cover the whole learning processes in this report.
- ***Automation***: Streamlining processes through automation, regardless of current complexity.
- ***Efficiency***: maintaining a clutter-free workspace including the removal of unnecessary objects. 

---

### Tools
- ***R Studio***: Main tool to clean, analyze, visualize data.
- ***Excel***: Prepare data as .csv files.
- ***Python***: Automating download of multiple .zip files.
- ***VS Code***: Editing Python file, maintaining files. 
- ***Git***: Syncing the process to Github.
- ***ChatGPT***: Answering technical questions quickly.
- ***Stackoverflow***: Searching technical questions.

---

## ASK
* ***Business Objective***: Cyclistic aimed to increase annual memberships based on understanding the differences in bike usage between annual members and casual riders.
* ***Key Questions***: Three guiding questions drove the analysis:
  * How do annual members and casual riders use Cyclistic bikes differently?
  * Why would casual riders buy Cyclistic annual memberships?
  * How can Cyclistic use digital media to influence casual riders to become members?
  
---

## PREPARE {.tabset}

### Data Source
The data are downloaded from [divvy](https://divvy-tripdata.s3.amazonaws.com/index.html) which is introduced by Coursera and Google. 
* In the [local version](https://github.com/Dongli99/Cyclistic.git) of the project, in order to download multiple files automatically, the [mutidownloader Python app](https://github.com/Dongli99/MultiDownloader.git) was developed under the assistant of ChatGPT.
* On [kaggle](https://www.kaggle.com/dongliai/cyclistic-analysis-workflow-using-r), a loop is created to gather and select the data spreaded in several datasets.
* This case study selects the subset of the data (2020Q1-202307).

| Data Overview    | Value                |
|------------------|----------------------|
| Data size        | 3.3G                 |
| Number of files  | 41                   |
| Number of columns| 13                   |
| Number of rows   | 17,962,572           |


> Data evaluation
  
  - credibility: high
  - problems:
    - distinct organization before 2020  -> only choose data after 2020
    - empty value on multiple columns -> investigate and clean in the following steps
    - duplicated rows -> investigate and clean in the following steps
  - usefulness to the business goal:
    - there are multiple variables can be analyze the different behaviors between member and casual.
    - It should be easy to get insights from the behaviors of the two groups.
    - The data is not promotion oriented. To achieve the goal, additional survey may be necessary. 

---

### Enviorment Prepare
```{r env_prepare}
install.packages("tidyverse")
install.packages("leaflet")
install.packages("ellipsis")
library(tidyverse)
library(skimr)
library(leaflet)
library(ellipsis)
```

---

### Data Prepare

> Set data path and file list.

```{r importing}
data_path <- "./tripdata"
csv_files <- list.files(data_path, pattern = "\\.csv", full.names = TRUE)
```

> Check col name consistency.

```{r colname_consistency, class.source = "reusable_module"}
check_column_consistency <- function(csv_files, std_cols) {
  inconsistent <- FALSE
  err_list <- character(0)
  for (file in csv_files){
    this_cols = colnames(read.csv(file))
    err_count = 0
    for (i in seq_along(this_cols)){
      if (this_cols[i] != std_cols[i]){
        err_list <- c(err_list, paste(file , "-" , this_cols[i]), " should be ", std_cols[i])
        err_count <- err_count + 1
        inconsistent <- TRUE
      }
    }
  }
  if (!inconsistent){
    print("The colnames are consistent")
  } else {
    print("WARNING - inconsistent cols:")
    print(paste(err_list, collapse = "\n"))
  }
}
std_cols <- colnames(read.csv(csv_files[1])) 
check_column_consistency(csv_files, std_cols)
```

> Combine .csv files into tibble

```{r combine_to_tb, class.source = "reusable_module"}
combine_csvs_to_tibble <- function(csv_files){
  tb = tibble() # declare an empty tibble
  for (file in csv_files){ # loop the path to rbind files to tb
    f <- read.csv(file)
    tb <- rbind(tb, f)
  } 
  return(tb)
}
tb <- combine_csvs_to_tibble(csv_files)
```

---

## PROCESS {.tabset}

### Inspect data

```{r inspect_tb}
skim_without_charts(tb)
```

---

### Transform data format

> Date Converting function

```{r convert_datetime, class.source = "reusable_module"}
convert_datetime <- function(column) {
  candidates <- c("%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M", "%d/%m/%Y %H:%M:%S", "%m/%d/%Y %H:%M:%S")
  out <- as.POSIXct(column, format = candidates[1])
  for (fmt in candidates[-1]) {
    if (!any(is.na(out))) break
    out[is.na(out)] <- as.POSIXct(column[is.na(out)], format = fmt)
  }
  return(out)
}
```

- Traditional way will miss at least 50% values due to the variety of date formats.
- This solution was inspired by [r2evans](https://stackoverflow.com/questions/70304425/r-datetime-series-missing-values)

> Mutate Data Types

```{r mutate_data_type}
tb <- tb %>% 
  mutate(
    start_station_id = as.factor(start_station_id),
    end_station_id = as.factor(end_station_id),
    start_station_name = as.factor(start_station_name),
    end_station_name = as.factor(end_station_name),
    rideable_type = as.factor(rideable_type),
    member_casual = as.factor(member_casual),
    ended_at = convert_datetime(ended_at),
    started_at = convert_datetime(started_at)
  )
```

---

### Improve Quality

> Trim abnormal Observations

```{r delete_abnormal}
tb <- tb %>% 
  filter(
    !is.na(start_station_id) & 
      !is.na(end_station_id) &
      !is.na(end_lat) & 
      !is.na(end_lng) &
      started_at < ended_at
    )
```

- less than 1% abnormal data were deleted

> Inspect geographic variable

- station_id, station_name, and lat&lng are three interchangeable geographic metrics
- lat&lng can be the measure of distance, and station id/name can be used for promotion
- This task is to inspect and optimize geographic data for future analysis

```{r inspect_station_id_name}
station_list <- tb %>% 
  select(start_station_id, start_station_name) %>%
  group_by(start_station_id, start_station_name) %>% 
  summarize(n = n(), .groups = 'drop') %>% 
  arrange(start_station_name)
station_list %>%
  filter(start_station_name == "")
```

- 11.3% (2015743) rows have empty station id & name. 
- Before cleaning station name, check the distribution of empty name.

```{r check_empty_name, class.source = "reusable_module", warning=FALSE}
empty_value_monthly <- function(tb, date_col, col_to_check1, col_to_check2){
  empty_list <- tb %>% 
  mutate(
    months = format(ymd_hms(.data[[date_col]]), "%Y-%m"),
    is_empty = ifelse(
      .data[[col_to_check1]]=="" | .data[[col_to_check2]]=="", 
      "Empty", 
      "Not empty"
      )
  )
  ggplot(data = empty_list)+
    geom_bar(mapping = aes(x = months, fill = is_empty))+
    theme_minimal()+
    theme(axis.text.x = element_text(angle = 45, size = 8))+
    scale_x_discrete(breaks = unique(empty_list$months)[c(TRUE, FALSE)])
}
empty_value_monthly(tb, "started_at", "start_station_name", "end_station_name")
```

- the empty values are evenly distributed, so the influence is considered limited.
- improve the data quality by deleting the rows with empty station names.

```{r remove_empty}
tb <- tb %>% 
  filter(start_station_name != "" & end_station_name != "")
```

---

### Refine 

> Rearrange columns

- remove unnecessary cols.
- add useful cols for further analysis.


```{r refine_dataset}
trips <- tb %>% 
  mutate(
    duration = as.integer((ended_at - started_at)/60),
    distance = round((sqrt((end_lat-start_lat)^2 + (end_lng-start_lng)^2))*1000000),
    weekday = as.factor(weekdays(started_at))
  ) %>% 
  mutate(
    is_weekend = ifelse(weekday %in% c("Saturday", "Sunday"), TRUE, FALSE)
  )
glimpse(trips)
```

---

## Analyze {.tabset}

### Group Summary

> In-depth comparable summary

- some insights can be initially detected by this function
- based on the summaries, hypothesis are made for further analysis

```{r group_summary, class.source = "reusable_module", warning=FALSE}
comparable_summaries <- function(df, col, groups){
  summary = data.frame()
  for (i in seq_along(groups)) {
    group_skim <- df %>% 
      filter(.data[[col]] == groups[i]) %>% 
      skim() %>% 
      mutate(
        group = groups[i]
      ) %>% 
      as_tibble()
    summary <- rbind(summary, group_skim)
  }
  sum <- df %>% 
    filter(.data[[col]] %in% groups) %>% 
    group_by(.data[[col]]) %>% 
    summarize(
      n_rows = n(), 
      percentage = n_rows / nrow(df) * 100
        ) %>% 
    print()
  print("------------------------------------------------------------------")
  nm_summary <- summary %>% 
    filter(!is.na(numeric.mean)) %>% 
    select(var = skim_variable, group, min = numeric.p0, max = numeric.p100, median = numeric.p50, mean = numeric.mean) %>% 
    group_by(var) %>% 
    print()
  print("------------------------------------------------------------------")
  chr_summary <- summary %>% 
    filter(!is.na(character.n_unique)) %>% 
    select(var = skim_variable, group, unique_values = character.n_unique) %>% 
    group_by(var) %>% 
    print()
  print("------------------------------------------------------------------")
  fct_summary <- summary %>% 
    filter(!is.na(factor.top_counts)) %>%
    select(var = skim_variable, group, count = factor.top_counts) %>% 
    group_by(var) %>% 
    print()
  print("------------------------------------------------------------------")
  lgc_summary <- summary %>% 
    filter(!is.na(logical.count)) %>% 
    select(var = skim_variable, group, count = logical.count) %>% 
    group_by(var) %>% 
    print()
}
comparable_summaries(trips, "member_casual", c("member", "casual"))
```

### Insights Based on the Comparable Summary
1. Casual riders are over 40%, showing high business potential.
2. Riding durations of casual are 2.44 times that of member, which is a big gap.
3. While the duration are significantly different, the distance of the two groups are similar.
4. Casual riders favorate riding in weekend, while the members usually ride in workdays. 
5. The stations of casuals and members visited most frequently are also distinct. A further geographical investigation is necessary to discover more.
6. Through the insights above, we may infer the pattern of casual riders. Many of them prefer riding in the weekend. Rather than taking bikes as a transpotation tool, they are more enjoying life.

### Most Popular Stations

```{r popular_stations}
start_stations <- trips %>%
  filter(member_casual=="casual") %>% 
  group_by(start_station_name) %>% 
  summarize(n = n(), .groups = 'drop') %>%
  rename(station_name = start_station_name)

end_stations <- trips %>%
  filter(member_casual=="casual") %>% 
  group_by(end_station_name) %>% 
  summarize(n = n(), .groups = 'drop') %>%
    rename(station_name = end_station_name)

stations <- rbind(start_stations, end_stations) %>%
  group_by(station_name) %>%
  summarise(n = sum(n), .groups = 'drop') %>% 
  arrange(-n) %>% 
  top_n(20, wt=n)

ggplot(data = stations) +
  geom_bar(mapping = aes(x=reorder(station_name, n), y=n), stat = "identity", position = "dodge") +
  coord_flip() +
  labs(title = "Top 20 popular Stations for Casual Users",
     x = "Station Name",
     y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
#### Most visited stations on map

```{r stations_map}
# prepare data
routes <- trips %>%
  group_by(member_casual, start_lng, start_lat, end_lng, end_lat) %>%
  summarize(n = n(), .groups="drop") %>%
  top_n(100, wt = n)

# General station distribution
leaflet(routes) %>% 
  setView(lng = -87.6298, lat = 41.8781, zoom = 10.5) %>% 
  addTiles() %>% 
  addProviderTiles(providers$CartoDB.Positron)%>%
  addCircleMarkers(
      ~start_lng,
      ~start_lat,
      radius = 9,
      color = ~ifelse(member_casual == "member", "red", "blue"),
      stroke = FALSE, fillOpacity = 0.5
  )%>%  
  addCircleMarkers(
      ~end_lng,
      ~end_lat,
      radius = 9,
      color = ~ifelse(member_casual == "member", "red", "blue"),
      stroke = FALSE, fillOpacity = 0.5
  ) 
```

  - Blue dots represents "casual user", red dots are for "member user". If one station is loved by both, it appears purple.
  - The two types of stations show different distributions.
  - Member's station distributed in clusters. In contrast, Casual user's stations are mostly along the lake as a line and all the way to the north.  
  
#### Weekday visitings

```{r weekday_routes}
# Compare busy stations in workdays and weekends
# prepare data
workday_routes <- trips %>%
  filter(!is_weekend)%>%
  group_by(member_casual, start_lng, start_lat, end_lng, end_lat) %>%
  summarize(n = n(), .groups="drop") %>%
  top_n(100, wt = n)

weekend_routes <- trips %>%
  filter(is_weekend)%>%
  group_by(member_casual, start_lng, start_lat, end_lng, end_lat) %>%
  summarize(n = n(), .groups="drop") %>%
  top_n(100, wt = n)

# plot routes map
print("Routes on Workdays")
leaflet(workday_routes) %>% 
  setView(lng = -87.6298, lat = 41.8781, zoom = 10.5) %>% 
  addTiles() %>% 
  addProviderTiles(providers$CartoDB.Positron)%>%
  addCircleMarkers(
      ~start_lng,
      ~start_lat,
      radius = 8,
      color = ~ifelse(member_casual == "member", "red", "blue"),
      stroke = TRUE, fillOpacity = 0
  )%>%  
  addCircleMarkers(
      ~end_lng,
      ~end_lat,
      radius = 8,
      color = ~ifelse(member_casual == "member", "red", "blue"),
      stroke = TRUE, fillOpacity = 0
  ) 

print("Routes in Weekend")
leaflet(weekend_routes) %>% 
  setView(lng = -87.6298, lat = 41.8781, zoom = 10.5) %>% 
  addTiles() %>% 
  addProviderTiles(providers$CartoDB.Positron)%>%
  addCircleMarkers(
      ~start_lng,
      ~start_lat,
      radius = 8,
      color = ~ifelse(member_casual == "member", "red", "blue"),
      stroke = TRUE, fillOpacity = 0
  )%>%  
  addCircleMarkers(
      ~end_lng,
      ~end_lat,
      radius = 8,
      color = ~ifelse(member_casual == "member", "red", "blue"),
      stroke = TRUE, fillOpacity = 0
  ) 
```

### History Ridings
 - In addition to geography, we need to investigate how the usage has changed over time.
 
#### Monthly trips trend

```{r history_rides}
library(lubridate)

# add year_months column
trips$year_month <- floor_date(trips$started_at, unit = "months")
user_counts <- trips %>%
  group_by(year_month, member_casual) %>%
  summarize(n = n(), .groups = "drop")

# plot the trend line
ggplot(user_counts, aes(x = year_month, y = n, color = member_casual)) +
  geom_line() +
  labs(x = "Year-Month", y = "Number of Rides") +
  scale_color_manual(values = c("blue", "red")) + 
  theme_minimal()
```
  - The casual utilization are stable and slightly growing in the 3 years.
  - The utilization appears strong seasonal volatility. Especially the casual user.
  - The number of casual riding usually drop to very low level in winter. But in summer, casual riding are almost at the same number with member's riding.
  - For casual user, annual membership may not be economic, and it should be split into monthly memberships.
  
### Riding Time Analysis
```{r riding_time}
# Create a sequence of hours
hours <- seq(0, 23, by = 1)

# Calculate the count of trips for each hour
busy_times <- trips %>%
  group_by(weekday, member_casual,  hour_start = floor(start_time), ) %>%
  summarize(count = n(), .groups = "drop")

# Create the line plot
ggplot(busy_times, aes(x = hour_start, y = count, color = member_casual)) +
  geom_line() +
  labs(x = "Hour of Day", y = "Number of Trips") +
  scale_color_manual(values = c("red", "blue")) +
  facet_wrap(~weekday, nrow = 7) +
  theme_minimal()
```
## SHARE AND ACT

### Insights

- Casual riders still make up a large percentage
  - More than 40% ridings are made by casual riders.
  - The portation of casual are stable and the number are slightly growing in the last 3 years.
- Casual riders show clear evidence to perform leisure tourism. 
  - They ride the similar distance in much longer time. 
  - Their favorate stations spread along the beach.
  - They strongly prefer riding in Weekend, and have the same time pattern with members.
  - On weekday, they tend to ride around evening rush hours.
  - Their number of rides is at the same or even higher level in Summer. But in winter, they ride much less.

### Pattern and Strategies
 
| Pattern            | Strategy                                          |
|--------------------|---------------------------------------------------|
| Inactive Winter    | Introduce **Monthly Membership**                   |
| Slow riding        | Gifting **free distance** rathan than free time    |
| Popular stations   | Conducting promotions at **top stations**           |
| Favorite time      | Rebates for **weekend** and worday **evening peak** |
| Purpose of leisure | **Relaxing** online and offline campaigns         |

# CONCLUSION

This is a really interesting dataset!    
The data looks huge and radom, but the patterns are so clear.
When I dig deeper, I found a lot of surpriseï¼Œand then, I found much more clues that may lead to more insights.
But I have to stop. To let my machine to have a rest, and to do the tons of assignments.

I really enjoy the journey. Especially when I created a reusable function and saw it worked, I feel like have saved tons of time in the future.

I hope you enjoy Cyclistic project as well.
Thank you for reading.

<style>
.reusable_module {
  background-color: lightblue;
}
</style>
 