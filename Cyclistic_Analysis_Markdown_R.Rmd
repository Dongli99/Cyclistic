---
title: "Cyclistic Analysis Workflow Using R"
author: "Dongli Liu"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(repos = "https://mirror.rcg.sfu.ca/mirror/CRAN/")
```

## INTRODUCTION {.tabset}

### Usage
> To recurrent:

  - pull [repository](https://github.com/Dongli99/Cyclistic.git)
  - download [dataset](https://centennialcollegeedu-my.sharepoint.com/:u:/g/personal/dliu99_my_centennialcollege_ca/EebPZIYzXXZAr0xgrzCrjkMBBJ1KGVEqkI1mHfyY7_SEqA?e=YmxcDV)
  - extract tripdata/ to Cyclistic/
  - run Cyclistic_Analysis_Markdown_R.Rmd in RStudio
  - expect long-time running due to large dataset.
  
> How to use 
```{normal_modules}
Normal code chunk.
```

```{reusable_module}
Reusable code chunk.
```

---

### Overview
This project focuses on the completion of the Google Data Analytics Certificate case study. The case study involved real-world data analysis tasks within a fictional company, Cyclistic, a bike-share program operating in Chicago. The goal was to analyze data, answer critical business questions, and make data-driven recommendations to maximize annual memberships.

---

### Process
The project followed a structured data analysis process:

- ***Ask***: Defined key business questions and objectives.
- ***Prepare***: Gathered and cleaned the data from various sources.
- ***Process***: Performed data manipulation and transformation for analysis.
- ***Analyze***: Conducted in-depth analysis to uncover insights.
- ***Share***: Communicated findings through visualizations and reports.
- ***Act***: Presented recommendations based on data insights.

---

### Principles
- ***Reusability***: Creating and collecting reusable modules for future applications.
- ***Optimization***: Continuously enhancing modules through study and work.
- ***Structured Organization***: Implementing an organized framework for knowledge and tools.
- ***Thoroughness***: Trying to cover the whole learning processes in this report.
- ***Automation***: Streamlining processes through automation, regardless of current complexity.
- ***Efficiency***: maintaining a clutter-free workspace including the removal of unnecessary objects. 

---

### Tools
- ***R Studio***: Main tool to clean, analyze, visualize data.
- ***Excel***: Prepare data as .csv files.
- ***Python***: Automating download of multiple .zip files.
- ***VS Code***: Editing Python file, maintaining files. 
- ***Git***: Syncing the process to Github.
- ***ChatGPT***: Answering technical questions quickly.
- ***Stackoverflow***: Searching technical questions.

---

## ASK
* ***Business Objective***: Cyclistic aimed to increase annual memberships and understand the differences in bike usage between annual members and casual riders.
* ***Key Questions***: Three guiding questions drove the analysis:
  * How do annual members and casual riders use Cyclistic bikes differently?
  * Why would casual riders buy Cyclistic annual memberships?
  * How can Cyclistic use digital media to influence casual riders to become members?
  
---

## PREPARE {.tabset}

### Data Source
The data are downloaded from [divvy](https://divvy-tripdata.s3.amazonaws.com/index.html) which is introduced by Coursera and Google. 
To download multiple files automatically, the [mutidownloader Python app](https://github.com/Dongli99/MultiDownloader.git) <span class="icon" title="Reusable Module">&#9851;</span> was developed under the assistant of ChatGPT.
The case study selects the subset of the data (2020Q1-202307).

> Data Overview

  - data size: 3.3G
  - num of files: 41
  - num of columns: 13
  - num of rows: 17,962,572

---

### Enviorment Prepare
```{r env_prepare}
install.packages("tidyverse")
library(tidyverse)
```

---

### Data Prepare

> Set data path and file list.

```{r importing}
data_path <- "./tripdata"
csv_files <- list.files(data_path, pattern = "\\.csv", full.names = TRUE)
```

> Check col name consistency.

```{r colname_consistency, class.source = "reusable_module"}
check_column_consistency <- function(csv_files, std_cols) {
  inconsistent <- FALSE
  err_list <- character(0)
  for (file in csv_files){
    this_cols = colnames(read.csv(file))
    err_count = 0
    for (i in seq_along(this_cols)){
      if (this_cols[i] != std_cols[i]){
        err_list <- c(err_list, paste(file , "-" , this_cols[i]), " should be ", std_cols[i])
        err_count <- err_count + 1
        inconsistent <- TRUE
      }
    }
  }
  if (!inconsistent){
    print("The colnames are consistent")
  } else {
    print("WARNING - inconsistent cols:")
    print(paste(err_list, collapse = "\n"))
  }
}
std_cols <- colnames(read.csv(csv_files[1])) 
check_column_consistency(csv_files, std_cols)
```

> Combine .csv files into tibble

```{r combine_to_tb, class.source = "reusable_module"}
combine_csvs_to_tibble <- function(csv_files){
  tb = tibble() # declare an empty tibble
  for (file in csv_files){ # loop the path to rbind files to tb
    f <- read.csv(file)
    tb <- rbind(tb, f)
  }
  return(tb)
}
tb <- combine_csvs_to_tibble(csv_files)
```

---

## PROCESS {.tabset}

### Inspect data

```{r inspect_tb}
library(skimr)
skim_without_charts(tb)
```

---

### Transform Format

> Date Converting function

```{r convert_datetime, class.source = "reusable_module"}
convert_datetime <- function(column) {
  candidates <- c("%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M", "%d/%m/%Y %H:%M:%S", "%m/%d/%Y %H:%M:%S")
  out <- as.POSIXct(column, format = candidates[1])
  for (fmt in candidates[-1]) {
    if (!any(is.na(out))) break
    out[is.na(out)] <- as.POSIXct(column[is.na(out)], format = fmt)
  }
  return(out)
}
```

- Traditional way will miss at least 50% values due to the variety of date formats.
- The solution was inspired by [r2evans](https://stackoverflow.com/questions/70304425/r-datetime-series-missing-values)

> Mutate Data Types

```{r mutate_data_type}
tb <- tb %>% 
  mutate(
    start_station_id = as.factor(start_station_id),
    end_station_id = as.factor(end_station_id),
    start_station_name = as.factor(start_station_name),
    end_station_name = as.factor(end_station_name),
    rideable_type = as.factor(rideable_type),
    member_casual = as.factor(member_casual),
    ended_at = convert_datetime(ended_at),
    started_at = convert_datetime(started_at)
  )
```

---

### Improve Quality

> Trim Incomplete Observations

```{r delete_incomplete}
tb <- tb %>% 
  filter(!is.na(start_station_id) & !is.na(end_station_id) & !is.na(end_lat) & !is.na(end_lng))
# less than 1% data lost after deleting
```

> Inspect Stations

- station_id, station_name, and lat&lng are three interchangeable geographic metrics
- This task is choose and optimize one geographic data for future analysis

```{r inspect_station_id_name}
station_list <- tb %>% 
  select(start_station_id, start_station_name) %>%
  group_by(start_station_id, start_station_name) %>% 
  summarize(n = n(), .groups = 'drop') %>% 
  arrange(start_station_name)
```

- 10% (2015929) rows have empty station id & name. 
- For better accuracy, seeking probability of using lng & lat as station.

```{r inspect_lat_lng}
lat_lng_list <- tb %>% 
  mutate(
    start_lat = round(start_lat, digits = 3),
    start_lng = round(start_lng, digits = 3),
    end_lat = round(end_lat, digits = 3),
    end_lng = round(end_lng, digits = 3)
  ) %>% 
  select(start_lat, start_lng) %>%
  group_by(start_lat, start_lng) %>% 
  summarize(n = n(), .groups = 'drop') %>% 
  arrange(start_lat)
```

- lat and lng are not accuracy enough
- Decide to use station name, check the distribution of empty name.

```{r check_empty_name, class.source = "reusable_module", warning=FALSE}
empty_value_monthly <- function(tb, date_col, col_to_check1, col_to_check2){
  empty_list <- tb %>% 
  mutate(
    months = format(ymd_hms(.data[[date_col]]), "%Y-%m"),
    is_empty = ifelse(
      .data[[col_to_check1]]=="" | .data[[col_to_check2]]=="", 
      "Empty", 
      "Not empty"
      )
  )
  ggplot(data = empty_list)+
    geom_bar(mapping = aes(x = months, fill = is_empty))+
    theme_minimal()+
    theme(axis.text.x = element_text(angle = 45, size = 8))+
    scale_x_discrete(breaks = unique(empty_list$months)[c(TRUE, FALSE)])
}
empty_value_monthly(tb, "started_at", "start_station_name", "end_station_name")
```

- The empty values are evenly distributed, so the influence is considered limited.
- Decide to delete the empty names.

```{r remove_empty}
tb <- tb %>% 
  filter(start_station_name != "" & end_station_name != "")
```


<style>
.icon {
  color: green;
  cursor: pointer;
}
.icon:hover::before {
  opacity: 1;
}  
.reusable_module {
  background-color: lightblue;
}
</style>
 