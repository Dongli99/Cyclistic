---
title: "Cyclistic Analysis Workflow Using R"
author: "Dongli Liu"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(repos = "https://mirror.rcg.sfu.ca/mirror/CRAN/")
```

## INTRODUCTION {.tabset}

### Usage
> To recurrent this case studay:
*Considering the size of the dataset, this version may be better to read only.*
*A [Kaggle version](https://www.kaggle.com/dongliai/cyclistic-analysis-workflow-using-r) is much easier to recurrent.*

  - to run this version locally, pull [repository](https://github.com/Dongli99/Cyclistic.git)
  - download [dataset](https://centennialcollegeedu-my.sharepoint.com/:u:/g/personal/dliu99_my_centennialcollege_ca/EebPZIYzXXZAr0xgrzCrjkMBBJ1KGVEqkI1mHfyY7_SEqA?e=YmxcDV)
  - extract tripdata/ to Cyclistic/
  - run Cyclistic_Analysis_Markdown_R.Rmd in RStudio
  - expect long-time running due to large dataset.
  
> How to use 

```{r normal_modules, eval=FALSE}
print("This is a normal code chunk.")
```

```{r reusable_module, class.source="reusable_module", eval=FALSE}
print("This is a reusable code chunk.")
```

---

### Overview
This project focuses on the completion of the [Google Data Analytics Professional Certificate](https://www.coursera.org/professional-certificates/google-data-analytics?) case study. The case study involved real-world data analysis tasks within a fictional company, Cyclistic, a bike-share program operating in Chicago. The goal was to analyze data, answer critical business questions, and make data-driven recommendations to maximize annual memberships.

---

### Process
The project followed a structured data analysis process:

- ***Ask***: Defined key business questions and objectives.
- ***Prepare***: Gathered and cleaned the data from various sources.
- ***Process***: Performed data manipulation and transformation for analysis.
- ***Analyze***: Conducted in-depth analysis to uncover insights.
- ***Share***: Communicated findings through visualizations and reports.
- ***Act***: Presented recommendations based on data insights.

---

### Principles
- ***Reusability***: Creating and collecting reusable modules for future applications.
- ***Optimization***: Continuously enhancing modules through study and work.
- ***Structured Organization***: Implementing an organized framework for knowledge and tools.
- ***Thoroughness***: Trying to cover the whole learning processes in this report.
- ***Automation***: Streamlining processes through automation, regardless of current complexity.
- ***Efficiency***: maintaining a clutter-free workspace including the removal of unnecessary objects. 

---

### Tools
- ***R Studio***: Main tool to clean, analyze, visualize data.
- ***Excel***: Prepare data as .csv files.
- ***Python***: Automating download of multiple .zip files.
- ***VS Code***: Editing Python file, maintaining files. 
- ***Git***: Syncing the process to Github.
- ***ChatGPT***: Answering technical questions quickly.
- ***Stackoverflow***: Searching technical questions.

---

## ASK
* ***Business Objective***: Cyclistic aimed to increase annual memberships based on understanding the differences in bike usage between annual members and casual riders.
* ***Key Questions***: Three guiding questions drove the analysis:
  * How do annual members and casual riders use Cyclistic bikes differently?
  * Why would casual riders buy Cyclistic annual memberships?
  * How can Cyclistic use digital media to influence casual riders to become members?
  
---

## PREPARE {.tabset}

### Data Source
The data are downloaded from [divvy](https://divvy-tripdata.s3.amazonaws.com/index.html) which is introduced by Coursera and Google. 
* In the [local version](https://github.com/Dongli99/Cyclistic.git) of the project, in order to download multiple files automatically, the [mutidownloader Python app](https://github.com/Dongli99/MultiDownloader.git) was developed under the assistant of ChatGPT.
* On [kaggle](https://www.kaggle.com/dongliai/cyclistic-analysis-workflow-using-r), a loop is created to gather and select the data spreaded in several datasets.
* This case study selects the subset of the data (2020Q1-202307).

| Data Overview    | Value                |
|------------------|----------------------|
| Data size        | 3.3G                 |
| Number of files  | 41                   |
| Number of columns| 13                   |
| Number of rows   | 17,962,572           |


> Data evaluation
  
  - credibility: high
  - problems:
    - distinct organization before 2020  -> only choose data after 2020
    - empty value on multiple columns -> investigate and clean in the following steps
    - duplicated rows -> investigate and clean in the following steps
  - usefulness to the business goal:
    - there are multiple variables can be analyze the different behaviors between member and casual.
    - It should be easy to get insights from the behaviors of the two groups.
    - The data is not promotion oriented. To achieve the goal, additional survey may be necessary. 

---

### Enviorment Prepare
```{r env_prepare}
install.packages("tidyverse")
library(tidyverse)
library(skimr)
```

---

### Data Prepare

> Set data path and file list.

```{r importing}
data_path <- "./tripdata"
csv_files <- list.files(data_path, pattern = "\\.csv", full.names = TRUE)
```

> Check col name consistency.

```{r colname_consistency, class.source = "reusable_module"}
check_column_consistency <- function(csv_files, std_cols) {
  inconsistent <- FALSE
  err_list <- character(0)
  for (file in csv_files){
    this_cols = colnames(read.csv(file))
    err_count = 0
    for (i in seq_along(this_cols)){
      if (this_cols[i] != std_cols[i]){
        err_list <- c(err_list, paste(file , "-" , this_cols[i]), " should be ", std_cols[i])
        err_count <- err_count + 1
        inconsistent <- TRUE
      }
    }
  }
  if (!inconsistent){
    print("The colnames are consistent")
  } else {
    print("WARNING - inconsistent cols:")
    print(paste(err_list, collapse = "\n"))
  }
}
std_cols <- colnames(read.csv(csv_files[1])) 
check_column_consistency(csv_files, std_cols)
```

> Combine .csv files into tibble

```{r combine_to_tb, class.source = "reusable_module"}
combine_csvs_to_tibble <- function(csv_files){
  tb = tibble() # declare an empty tibble
  for (file in csv_files){ # loop the path to rbind files to tb
    f <- read.csv(file)
    tb <- rbind(tb, f)
  } 
  return(tb)
}
tb <- combine_csvs_to_tibble(csv_files)
```

---

## PROCESS {.tabset}

### Inspect data

```{r inspect_tb}
skim_without_charts(tb)
```

---

### Transform data format

> Date Converting function

```{r convert_datetime, class.source = "reusable_module"}
convert_datetime <- function(column) {
  candidates <- c("%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M", "%d/%m/%Y %H:%M:%S", "%m/%d/%Y %H:%M:%S")
  out <- as.POSIXct(column, format = candidates[1])
  for (fmt in candidates[-1]) {
    if (!any(is.na(out))) break
    out[is.na(out)] <- as.POSIXct(column[is.na(out)], format = fmt)
  }
  return(out)
}
```

- Traditional way will miss at least 50% values due to the variety of date formats.
- This solution was inspired by [r2evans](https://stackoverflow.com/questions/70304425/r-datetime-series-missing-values)

> Mutate Data Types

```{r mutate_data_type}
tb <- tb %>% 
  mutate(
    start_station_id = as.factor(start_station_id),
    end_station_id = as.factor(end_station_id),
    start_station_name = as.factor(start_station_name),
    end_station_name = as.factor(end_station_name),
    rideable_type = as.factor(rideable_type),
    member_casual = as.factor(member_casual),
    ended_at = convert_datetime(ended_at),
    started_at = convert_datetime(started_at)
  )
```

---

### Improve Quality

> Trim abnormal Observations

```{r delete_abnormal}
tb <- tb %>% 
  filter(
    !is.na(start_station_id) & 
      !is.na(end_station_id) &
      !is.na(end_lat) & 
      !is.na(end_lng) &
      started_at < ended_at
    )
```

- less than 1% abnormal data were deleted

> Inspect geographic variable

- station_id, station_name, and lat&lng are three interchangeable geographic metrics
- lat&lng can be the measure of distance, and station id/name can be used for promotion
- This task is to inspect and optimize geographic data for future analysis

```{r inspect_station_id_name}
station_list <- tb %>% 
  select(start_station_id, start_station_name) %>%
  group_by(start_station_id, start_station_name) %>% 
  summarize(n = n(), .groups = 'drop') %>% 
  arrange(start_station_name)
station_list %>%
  filter(start_station_name == "")
```

- 11.3% (2015743) rows have empty station id & name. 
- Before cleaning station name, check the distribution of empty name.

```{r check_empty_name, class.source = "reusable_module", warning=FALSE}
empty_value_monthly <- function(tb, date_col, col_to_check1, col_to_check2){
  empty_list <- tb %>% 
  mutate(
    months = format(ymd_hms(.data[[date_col]]), "%Y-%m"),
    is_empty = ifelse(
      .data[[col_to_check1]]=="" | .data[[col_to_check2]]=="", 
      "Empty", 
      "Not empty"
      )
  )
  ggplot(data = empty_list)+
    geom_bar(mapping = aes(x = months, fill = is_empty))+
    theme_minimal()+
    theme(axis.text.x = element_text(angle = 45, size = 8))+
    scale_x_discrete(breaks = unique(empty_list$months)[c(TRUE, FALSE)])
}
empty_value_monthly(tb, "started_at", "start_station_name", "end_station_name")
```

- the empty values are evenly distributed, so the influence is considered limited.
- improve the data quality by deleting the rows with empty station names.

```{r remove_empty}
tb <- tb %>% 
  filter(start_station_name != "" & end_station_name != "")
```

---

### Refine 

> Rearrange columns

- remove unnecessary cols.
- add useful cols for further analysis.


```{r refine_dataset}
trips <- tb %>% 
  mutate(
    duration = as.integer((ended_at - started_at)/60),
    distance = round((sqrt((end_lat-start_lat)^2 + (end_lng-start_lng)^2))*1000000),
    weekday = as.factor(weekdays(started_at))
  ) %>% 
  mutate(
    is_weekend = ifelse(weekday %in% c("Saturday", "Sunday"), TRUE, FALSE)
  )
glimpse(trips)
```

---

## Analyze {.tabset}

### Group Summary

> In-depth comparable summary

- some insights can be initially detected by this function
- based on the summaries, hypothesis are made for further analysis

```{r group_summary, class.source = "reusable_module", warning=FALSE}
comparable_summaries <- function(df, col, groups){
  summary = data.frame()
  for (i in seq_along(groups)) {
    group_skim <- df %>% 
      filter(.data[[col]] == groups[i]) %>% 
      skim() %>% 
      mutate(
        group = groups[i]
      ) %>% 
      as_tibble()
    summary <- rbind(summary, group_skim)
  }
  sum <- df %>% 
    filter(.data[[col]] %in% groups) %>% 
    group_by(.data[[col]]) %>% 
    summarize(
      n_rows = n(), 
      percentage = n_rows / nrow(df) * 100
        ) %>% 
    print()
  print("------------------------------------------------------------------")
  nm_summary <- summary %>% 
    filter(!is.na(numeric.mean)) %>% 
    select(var = skim_variable, group, min = numeric.p0, max = numeric.p100, median = numeric.p50, mean = numeric.mean) %>% 
    group_by(var) %>% 
    print()
  print("------------------------------------------------------------------")
  chr_summary <- summary %>% 
    filter(!is.na(character.n_unique)) %>% 
    select(var = skim_variable, group, unique_values = character.n_unique) %>% 
    group_by(var) %>% 
    print()
  print("------------------------------------------------------------------")
  fct_summary <- summary %>% 
    filter(!is.na(factor.top_counts)) %>%
    select(var = skim_variable, group, count = factor.top_counts) %>% 
    group_by(var) %>% 
    print()
  print("------------------------------------------------------------------")
  lgc_summary <- summary %>% 
    filter(!is.na(logical.count)) %>% 
    select(var = skim_variable, group, count = logical.count) %>% 
    group_by(var) %>% 
    print()
}
comparable_summaries(trips, "member_casual", c("member", "casual"))
```

### Insights Based on the Comparable Summary
1. Casual riders are over 40%, showing high business potential.
2. Riding durations of casual are 2.44 times that of member, which is a big gap.
3. While the duration are significantly different, the distance of the two groups are similar.
4. Casual riders favorate riding in weekend, while the members usually ride in workdays. 
5. The stations of casuals and members visited most frequently are also distinct. A further geographical investigation is necessary to discover more.
6. Through the insights above, we may infer the pattern of casual riders. Many of them prefer riding in the weekend. Rather than taking bikes as a transpotation tool, they are more enjoying life.

### Most Popular Stations

```{r popular_stations}
start_stations <- trips %>%
  filter(member_casual=="casual") %>% 
  group_by(start_station_name) %>% 
  summarize(n = n(), .groups = 'drop') %>%
  rename(station_name = start_station_name)

end_stations <- trips %>%
  filter(member_casual=="casual") %>% 
  group_by(end_station_name) %>% 
  summarize(n = n(), .groups = 'drop') %>%
    rename(station_name = end_station_name)

stations <- rbind(start_stations, end_stations) %>%
  group_by(station_name) %>%
  summarise(n = sum(n), .groups = 'drop') %>% 
  arrange(-n) %>% 
  top_n(20, wt=n)

ggplot(data = stations) +
  geom_bar(mapping = aes(x=reorder(station_name, n), y=n), stat = "identity", position = "dodge") +
  coord_flip() +
  labs(title = "Top 20 popular Stations for Casual Users",
     x = "Station Name",
     y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
### Most visited Locations on google map
*This section is only available at [kaggle version](https://www.kaggle.com/dongliai/cyclistic-analysis-workflow-using-r)*
```{r}
# Assuming 'routes' data frame is already calculated
# routes <- ... (your code to calculate routes)

routes <- trips %>%
  group_by(member_casual, start_lng, start_lat, end_lng, end_lat) %>%
  reframe(group = member_casual, n = n())
print(routes)

```


<style>
.reusable_module {
  background-color: lightblue;
}
</style>
 